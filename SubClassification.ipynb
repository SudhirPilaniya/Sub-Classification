{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wevOqbbd1kLC",
        "outputId": "5861c0a7-830c-409d-e0f8-1a9cbca921eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n",
        "import string\n",
        "string.punctuation\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GlobalAveragePooling1D\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MdYJNPH4y1UQ",
        "outputId": "da6b23f7-5235-482f-f2c8-3257154a6f63"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/subjects-questions.csv')\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KemNlsKq3RB_",
        "outputId": "5b958f5f-8c99-42b2-dae1-9c79952c271c"
      },
      "outputs": [],
      "source": [
        "df.Subject.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtttxA3gV_V9",
        "outputId": "95680838-2867-4e65-a6e6-db613208c05a"
      },
      "outputs": [],
      "source": [
        "df['Subject'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYRvKtj3Zpq",
        "outputId": "4254d145-f752-4f89-a6fb-316f813ab42e"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "PIIrnzdSCyFP",
        "outputId": "7b8d4078-d54d-48e1-9540-3b890d48dbc2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8, 8))\n",
        "sns.countplot(df['Subject'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3bCk6Ylk3bk2",
        "outputId": "2ea9c899-8555-4325-d542-0ff24f60566f"
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4BfJzvXl3dQf",
        "outputId": "f6a80bd9-b1e6-4104-d948-5fe01a020ff4"
      },
      "outputs": [],
      "source": [
        "category = pd.get_dummies(df.Subject)\n",
        "df = pd.concat([df, category], axis = 1)\n",
        "df = df.drop(columns = 'Subject')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Pr0kF12W3hfI",
        "outputId": "d3160184-c724-49fb-dfe5-655d4d62b763"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHf6n4t636X9"
      },
      "outputs": [],
      "source": [
        "def remove_Stopwords(text ):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize( text.lower() )\n",
        "    sentence = [w for w in words if not w in stop_words]\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    wordlist=[]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentences=sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words=word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            wordlist.append(lemmatizer.lemmatize(word))\n",
        "    return ' '.join(wordlist)\n",
        "\n",
        "def clean_text(text ):\n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation}\n",
        "    delete_dict[' '] = ' '\n",
        "    table = str.maketrans(delete_dict)\n",
        "    text1 = text.translate(table)\n",
        "    textArr= text1.split()\n",
        "    text2 = ' '.join([w for w in textArr])\n",
        "\n",
        "    return text2.lower()\n",
        "\n",
        "def stemSentence(text):\n",
        "    porter = PorterStemmer()\n",
        "    token_words=word_tokenize(text)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BF5vicre4KMd",
        "outputId": "4f5c94be-b0a8-4140-d601-750194dd7028"
      },
      "outputs": [],
      "source": [
        "df['eng'] = df['eng'].apply(remove_Stopwords)\n",
        "df['eng'] = df['eng'].apply(lemmatize_text)\n",
        "df['eng'] = df['eng'].apply(clean_text)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLI94ew-4QKm"
      },
      "outputs": [],
      "source": [
        "df['eng'] = df['eng'].apply(stemSentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tAjmFk_Q7u4O",
        "outputId": "ce273a61-7ac5-4668-e195-85b2e8e7425f"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8TGSBzA8G2L",
        "outputId": "8cfbc489-3c15-425a-b0ed-2f48a3bde7e3"
      },
      "outputs": [],
      "source": [
        "length = df['eng'].str.len().max()\n",
        "length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDKEs5VR8ZvD",
        "outputId": "9ff4d5a2-6483-4c70-9874-7ac4d130035d"
      },
      "outputs": [],
      "source": [
        "ques = df['eng'].values\n",
        "subs = df[['Biology', 'Chemistry', 'Maths', 'Physics']].values\n",
        "subs, ques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gakgeAUD8J6"
      },
      "outputs": [],
      "source": [
        "ques_train, ques_test, subs_train, subs_test = train_test_split(ques, subs, test_size = 0.2, random_state = 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtD4gHiQEcmM",
        "outputId": "50322e04-dcf2-437c-8bd4-d585e7e7d6bf"
      },
      "outputs": [],
      "source": [
        "ques_train.shape , ques_test.shape , subs_train.shape , subs_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR2PBfiwEJxf",
        "outputId": "2ff8076a-33b6-4a07-f96b-b285565118ab"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words = length, oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(ques_train)\n",
        "tokenizer.fit_on_texts(ques_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(ques_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(ques_test)\n",
        "\n",
        "padded_train = pad_sequences(sequences_train,\n",
        "                             maxlen = 5,\n",
        "                             padding = 'post',\n",
        "                             truncating = 'post')\n",
        "padded_test = pad_sequences(sequences_test,\n",
        "                            maxlen = 5,\n",
        "                            padding = 'post',\n",
        "                            truncating = 'post')\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size),\n",
        "padded_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hELTOwNWEV-6"
      },
      "outputs": [],
      "source": [
        "# model=Sequential()\n",
        "# model.add(Embedding(input_dim=98015,output_dim=64))\n",
        "# model.add(LSTM(64))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(16, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(8, activation=('relu')))\n",
        "# model.add(Dense(4, activation=('softmax')))\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Embedding(input_dim=10000, output_dim=16),)\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(GlobalAveragePooling1D())\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation= 'relu'))\n",
        "model.add(Dense(4, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snwGsGVgFRcz"
      },
      "outputs": [],
      "source": [
        "model.compile(loss ='categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsIsUp0AFtdZ",
        "outputId": "6547a272-882d-4e69-a85c-222548efd286"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-IGCRWXFwLG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "\n",
        "# class accCallback(Callback):\n",
        "#    def on_epoch_end(self, epoch, logs={}):\n",
        "#         if(logs.get('accuracy') >= 0.98 and logs.get('val_accuracy') >= 0.98):\n",
        "#             print(\"\\nAccuracy and Val_Accuracy has reached 90%!\", \"\\nEpoch: \", epoch)\n",
        "#             self.model.stop_training = True\n",
        "\n",
        "# callbacks = accCallback()\n",
        "\n",
        "earlystopping = EarlyStopping(\n",
        "    monitor = 'val_accuracy',\n",
        "    min_delta = 0,\n",
        "    patience = 4,\n",
        "    verbose = 1,\n",
        "    mode = 'auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FNcWqltGJWV",
        "outputId": "32dcee9d-c3cb-4974-ed88-c2f386dd8faa"
      },
      "outputs": [],
      "source": [
        "history = model.fit(padded_train, subs_train,steps_per_epoch = 30,epochs = 100,validation_data = (padded_test, subs_test),\n",
        "                    verbose = 1,validation_steps = 50,callbacks=[earlystopping] , batch_size = 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoFMgz_nIUj_",
        "outputId": "1dd1763b-7942-49d5-c9a8-fc456caae55d"
      },
      "outputs": [],
      "source": [
        "# from keras.models import load_model\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import numpy as np\n",
        "\n",
        "# Function to preprocess text\n",
        "# def preprocess_text(text):\n",
        "#     tokens = word_tokenize(text.lower())\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "#     return tokens\n",
        "\n",
        "# Load the model\n",
        "# model = load_model(\"model.h5\")\n",
        "\n",
        "# Sample custom question\n",
        "custom_question = \"area two similar triangl equal equilater b iso...\t\"\n",
        "\n",
        "# Preprocess the custom question\n",
        "# preprocessed_question = preprocess_text(custom_question)\n",
        "\n",
        "#Start\n",
        "\n",
        "custom_question = remove_Stopwords(custom_question)\n",
        "custom_question = lemmatize_text(custom_question)\n",
        "custom_question = clean_text(custom_question)\n",
        "custom_question = stemSentence(custom_question)\n",
        "\n",
        "tokenizer.fit_on_texts(custom_question)\n",
        "custom_question = tokenizer.texts_to_sequences([custom_question])\n",
        "\n",
        "padded_cq = pad_sequences(custom_question,maxlen = 5,padding = 'post',truncating = 'post')\n",
        "\n",
        "#End\n",
        "\n",
        "# Tokenize and pad the sequence\n",
        "# sequences = tokenizer.texts_to_sequences([preprocessed_question])\n",
        "# padded_sequence = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_cq)\n",
        "\n",
        "# Decode predictions (assuming you used one-hot encoding for training)\n",
        "classes = ['biology', 'chemistry', 'maths', 'physics']\n",
        " # Replace with your actual class labels\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "predicted_class = classes[predicted_class_index]\n",
        "#  classes[predicted_class_index]\n",
        "\n",
        "# Display the results\n",
        "print(f\"Custom Question: {custom_question}\")\n",
        "print(predictions)\n",
        "print(f\"Predicted Class: {predicted_class}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
